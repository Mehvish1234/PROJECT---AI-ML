# -*- coding: utf-8 -*-
"""Modeltraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hcpnovIG7QQE1tmQP_Zqas74zvulQdmn
"""

!pip install tensorflow opencv-python-headless matplotlib

import cv2
import numpy as np
import os
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

from tensorflow.keras.preprocessing.image import ImageDataGenerator

img_size = 100
batch_size = 32

datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='binary',
    subset='training'
)



val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='binary',
    subset='validation'
)

!pip install streamlit
!pip install pyngrok

!pip install streamlit pyngrok --quiet

!ngrok config add-authtoken "2yiTJ3E8GQcBFle4sfojDY0AdyA_522nYpXuceTUizEA81u42"

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from tensorflow.keras.models import load_model
# from tensorflow.keras.preprocessing import image
# import numpy as np
# 
# model = load_model('face_mask_model.h5')  # Make sure this file is in Colab
# 
# st.title("Face Mask Detection App")
# 
# uploaded_file = st.file_uploader("Upload an Image", type=["jpg", "png", "jpeg"])
# if uploaded_file is not None:
#     img = image.load_img(uploaded_file, target_size=(100, 100))
#     st.image(img, caption="Uploaded Image", use_column_width=True)
# 
#     img_array = image.img_to_array(img) / 255.0
#     img_array = np.expand_dims(img_array, axis=0)
# 
#     prediction = model.predict(img_array)[0][0]
#     label = "üò∑ Mask Detected" if prediction < 0.5 else "‚ùå No Mask Detected"
#     st.write("## Prediction:", label)
#

from pyngrok import ngrok

# Start the tunnel
public_url = ngrok.connect(8501)
print("üöÄ Public URL:", public_url)

# Start streamlit in background
!streamlit run app.py &>/dev/null &

from google.colab import drive
drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/archive (4)'

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = datagen.flow_from_directory(
    dataset_path,         # üëà use full path
    target_size=(100, 100),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(100, 100),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

print(train_generator.class_indices)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 3)),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Binary classification
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10
)

model.save("face_mask_model.h5")

from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt

img_path = '/content/without_mask_9.jpg'  # path to your test image
img = image.load_img(img_path, target_size=(100, 100))
plt.imshow(img)
plt.axis('off')
plt.title("Test Image")
plt.show()

img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

prediction = model.predict(img_array)[0][0]

# Remember your classes:
# {'with_mask': 0, 'without_mask': 1}
label = "With Mask üò∑" if prediction < 0.5 else "No Mask ‚ùå"
print("Prediction:", label)

pip install playsound opencv-python

import os

def beep():
    # Windows
    if os.name == 'nt':
        import winsound
        winsound.Beep(1000, 300)  # frequency, duration
    else:
        # Mac/Linux
        os.system('beep -f 1000 -l 300')

import cv2
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array
import numpy as np
import os

model = load_model("face_mask_model.h5")
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

def beep():
    if os.name == 'nt':
        import winsound
        winsound.Beep(1000, 300)
    else:
        os.system('beep -f 1000 -l 300')

cap = cv2.VideoCapture(0)

while True:
    _, frame = cap.read()
    faces = face_cascade.detectMultiScale(frame, 1.1, 4)

    for (x, y, w, h) in faces:
        face = frame[y:y+h, x:x+w]
        face = cv2.resize(face, (100, 100))
        face = img_to_array(face) / 255.0
        face = np.expand_dims(face, axis=0)

        pred = model.predict(face)[0][0]
        label = "With Mask" if pred < 0.5 else "No Mask"
        color = (0, 255, 0) if label == "With Mask" else (0, 0, 255)

        # Draw rectangle + label
        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

        # Trigger beep alert if no mask
        if label == "No Mask":
            beep()

    cv2_imshow(frame)


    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

from google.colab.patches import cv2_imshow



from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import img_to_array
import numpy as np
import cv2
from google.colab.patches import cv2_imshow
from google.colab import files

# Load your trained model
model = load_model("/content/face_mask_model.h5")

# Upload an image
uploaded = files.upload()
img_path = list(uploaded.keys())[0]

# Load and preprocess the image
image = cv2.imread(img_path)
image_resized = cv2.resize(image, (100, 100))
img_array = img_to_array(image_resized) / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Predict
prediction = model.predict(img_array)[0][0]
label = "With Mask üò∑" if prediction < 0.5 else "No Mask ‚ùå"
color = (0, 255, 0) if prediction < 0.5 else (0, 0, 255)

# Add visual alert
cv2.putText(image, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
cv2.rectangle(image, (0, 0), (image.shape[1], image.shape[0]), color, 5)

# Show result
cv2_imshow(image)
print("Prediction:", label)

from tensorflow.keras.models import load_model
model = load_model('face_mask_model.h5')

from google.colab import files
import cv2
import numpy as np
from tensorflow.keras.preprocessing.image import img_to_array
from google.colab.patches import cv2_imshow

# Upload image
uploaded = files.upload()
img_path = list(uploaded.keys())[0]

# Load image
image = cv2.imread(img_path)
image_resized = cv2.resize(image, (100, 100))  # use same size as training
img_array = image_resized / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Predict
prediction = model.predict(img_array)[0][0]
print("üîç Raw prediction score:", prediction)

# Label & confidence
label = "With Mask üò∑" if prediction < 0.5 else "No Mask ‚ùå"
confidence = round((1 - prediction) * 100, 2) if prediction < 0.5 else round(prediction * 100, 2)

# Draw result on image
color = (0, 255, 0) if label == "With Mask üò∑" else (0, 0, 255)
cv2.putText(image, f"{label} ({confidence}%)", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
cv2.rectangle(image, (0, 0), (image.shape[1], image.shape[0]), color, 5)

# Show image
cv2_imshow(image)

# Final output
print(f"‚úÖ Prediction: {label}")
print(f"üìä Confidence: {confidence}%")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

img_size = 224  # MobileNetV2 expects 224x224

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    brightness_range=[0.7, 1.3],
    horizontal_flip=True,
    validation_split=0.2  # Use 20% data for validation
)

train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/archive (4)',
    target_size=(img_size, img_size),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/archive (4)',
    target_size=(img_size, img_size),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam

base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))
base_model.trainable = False  # Freeze base

# Add custom layers on top
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.3)(x)
predictions = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=predictions)

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10  # You can increase to 15‚Äì20
)

model.save("mobilenet_face_mask_model.h5")

base_model.trainable = True

model.compile(optimizer=Adam(1e-5),  # Lower LR for fine-tuning
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(train_generator, validation_data=val_generator, epochs=5)

pip install tensorflow keras opencv-python playsound

import cv2
import numpy as np
from keras.models import load_model
from playsound import playsound
import threading

# Load model
model = load_model('/content/drive/MyDrive/face_mask_model/face_mask_model.h5')

# Load Haar Cascade
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Play buzzer sound (non-blocking)
def play_buzzer():
    threading.Thread(target=playsound, args=('/content/buzz-buzz-95806.mp3',), daemon=True).start()

# Start video stream
cap = cv2.VideoCapture(0)

labels_dict = {0: 'Mask', 1: 'No Mask'}
color_dict = {0: (0,255,0), 1: (0,0,255)}

while True:
    ret, frame = cap.read()
    if not ret:
        break

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)

    for x, y, w, h in faces:
        face_img = frame[y:y+h, x:x+w]
        resized = cv2.resize(face_img, (128,128))  # use model input size
        normalized = resized / 255.0
        reshaped = np.reshape(normalized, (1,128,128,3))  # change shape as per model

        result = model.predict(reshaped)
        label = np.argmax(result, axis=1)[0]  # 0 or 1

        # Draw rectangle & label
        cv2.rectangle(frame, (x,y), (x+w,y+h), color_dict[label], 2)
        cv2.putText(frame, labels_dict[label], (x, y-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color_dict[label], 2)

        # Trigger buzzer if No Mask
        if label == 1:
            play_buzzer()

    cv2.imshow('Face Mask Detector', frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()

from flask import Flask, render_template, Response
import cv2
import numpy as np
from keras.models import load_model
from playsound import playsound
import threading

app = Flask(__name__)

# Load model
model = load_model("/content/drive/MyDrive/face_mask_model/face_mask_model.h5")
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

labels_dict = {0: "Mask", 1: "No Mask"}
color_dict = {0: (0, 255, 0), 1: (0, 0, 255)}

def play_buzzer():
    threading.Thread(target=playsound, args=("/content/buzz-buzz-95806.mp3",), daemon=True).start()

def gen_frames():
    cap = cv2.VideoCapture(0)
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        for x, y, w, h in faces:
            face = frame[y:y+h, x:x+w]
            resized = cv2.resize(face, (128,128))
            reshaped = np.reshape(resized / 255.0, (1,128,128,3))

            result = model.predict(reshaped)
            label = np.argmax(result, axis=1)[0]

            cv2.rectangle(frame, (x, y), (x+w, y+h), color_dict[label], 2)
            cv2.putText(frame, labels_dict[label], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color_dict[label], 2)

            if label == 1:
                play_buzzer()

        ret, buffer = cv2.imencode('.jpg', frame)
        frame = buffer.tobytes()

        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n')

    cap.release()

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/video_feed')
def video_feed():
    return Response(gen_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

if __name__ == "__main__":
    app.run(debug=True)

from IPython.display import display, Javascript
from google.colab.output import eval_js
from IPython.display import Image
import cv2
import numpy as np

def capture_image():
    js = Javascript('''
        async function takePhoto() {
          const div = document.createElement('div');
          const capture = document.createElement('button');
          capture.textContent = 'Capture';
          div.appendChild(capture);

          const video = document.createElement('video');
          video.style.display = 'block';
          const stream = await navigator.mediaDevices.getUserMedia({video: true});

          document.body.appendChild(div);
          div.appendChild(video);
          video.srcObject = stream;
          await video.play();

          // Resize the output to fit the video element.
          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

          // Wait for Capture to be clicked.
          await new Promise((resolve) => capture.onclick = resolve);

          const canvas = document.createElement('canvas');
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          canvas.getContext('2d').drawImage(video, 0, 0);
          stream.getTracks().forEach(track => track.stop());
          div.remove();
          return canvas.toDataURL('image/jpeg', 0.8);
        }
        takePhoto();
    ''')
    display(js)
    data = eval_js("takePhoto()")
    binary = b''.join([bytes([ord(x)]) for x in data.split(',')[1]])
    with open("photo.jpg", "wb") as f:
        f.write(binary)

    return "photo.jpg"

from keras.models import load_model

model = load_model('/content/drive/MyDrive/face_mask_model/improved_face_mask_model.h5')

def take_photo(return_image=True):
    from IPython.display import display, Javascript
    from google.colab.output import eval_js
    import base64
    import io
    from PIL import Image

    js = Javascript('''
      async function takePhoto() {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'üì∏ Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        video.style.display = 'block';
        const stream = await navigator.mediaDevices.getUserMedia({video: true});
        div.appendChild(video);
        document.body.appendChild(div);
        video.srcObject = stream;
        await video.play();

        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);

        stream.getTracks().forEach(track => track.stop());
        div.remove();

        return canvas.toDataURL('image/jpeg', 0.8);
      }
      takePhoto();
    ''')
    display(js)

    data = eval_js("takePhoto()")
    header, encoded = data.split(",", 1)
    binary = base64.b64decode(encoded)

    image = Image.open(io.BytesIO(binary))
    image.save("photo.jpg")

    if return_image:
        return image
    else:
        return "photo.jpg"

image = take_photo()
image.show()  # Should display the captured photo in Colab output

# Load model
from keras.models import load_model
model = load_model("/content/drive/MyDrive/fine_tuned_face_mask_model.h5", compile=False)

# Resize to match model input (224x224), normalize, reshape
img = image.resize((224, 224))
img = np.array(img) / 255.0
img = np.reshape(img, (1, 224, 224, 3))

# Predict
pred = model.predict(img)
label = np.argmax(pred)

# Show result
if label == 0:
    print("‚úÖ MASK DETECTED")
else:
    print("‚ùå NO MASK DETECTED")
    from IPython.display import Audio
    display(Audio("https://www.soundjay.com/button/beep-07.wav", autoplay=True))

from keras.models import load_model

model = load_model('/content/drive/MyDrive/face_mask_model/face_mask_model.h5', compile=False)  # Don't compile yet

import os

# Check that dataset exists
os.listdir("/content/drive/MyDrive/dataset")

from keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 224  # Make sure this matches your model's expected input

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

train_generator = datagen.flow_from_directory(
    'dataset',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    'dataset',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

from tensorflow.keras.models import load_model

model = load_model("/content/drive/MyDrive/face_mask_model/face_mask_model.h5", compile=False)
print("Model input shape:", model.input_shape)

# ‚úÖ Step 1: Install and Import Required Libraries
!pip install tensorflow keras --quiet

import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam

# ‚úÖ Step 2: Load the Pretrained Model (.h5)
model = load_model("/content/drive/MyDrive/face_mask_model/face_mask_model.h5", compile=False)

# ‚úÖ Step 3: Detect Input Shape Automatically
input_shape = model.input_shape
IMG_SIZE = input_shape[1]  # Example: 224 if input shape is (None, 224, 224, 3)

print(f"‚úÖ Model input shape: {input_shape}")
print(f"‚úÖ Using image size: {IMG_SIZE}x{IMG_SIZE}")

# ‚úÖ Step 4: Setup Data Generators (Assumes dataset/with_mask and dataset/without_mask)
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    horizontal_flip=True,
    zoom_range=0.2,
    rotation_range=15
)

train_generator = datagen.flow_from_directory(
    '/content/drive/MyDrive/dataset',  # ‚úÖ Make sure folder exists
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    '/content/drive/MyDrive/dataset',  # Same path
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)


# ‚úÖ Step 5: Compile the Model for Training
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)


# ‚úÖ Step 6: Retrain the Model
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=5,  # You can increase to 10 or 20
    steps_per_epoch=train_generator.samples // 32,
    validation_steps=val_generator.samples // 32
)

# ‚úÖ Step 7: Save the Updated Model
model.save("face_mask_model_updated.h5")
print("‚úÖ Model saved as face_mask_model_updated.h5")

# ‚úÖ Step 8: Plot Training History
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Define dataset path
dataset_path = "/content/drive/MyDrive/dataset"

# Load pre-trained model (compile=False to avoid warnings)
model = load_model("/content/drive/MyDrive/face_mask_model/face_mask_model.h5", compile=False)

# Detect model input shape (should be like (None, 224, 224, 3))
input_shape = model.input_shape
IMG_SIZE = input_shape[1]

print(f"Model expects image size: {IMG_SIZE}x{IMG_SIZE}")

# Use binary class_mode since model outputs a single sigmoid value
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=15,
    zoom_range=0.2,
    horizontal_flip=True
)

train_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

val_generator = datagen.flow_from_directory(
    dataset_path,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,  # Increase this if needed
    steps_per_epoch=train_generator.samples // 32,
    validation_steps=val_generator.samples // 32
)

model.save("/content/drive/MyDrive/face_mask_model/face_mask_model_updated.h5")
print("‚úÖ Updated model saved as face_mask_model_updated.h5")

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()